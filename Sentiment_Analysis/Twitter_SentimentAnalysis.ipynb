{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import plotly.express as px\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "host_name = os.getenv('HOST_NAME')\n",
    "passwd = os.getenv('AWS_PASSWORD')\n",
    "user='group2'\n",
    "database='group2db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting AWS mysql Database\n",
    "con = pymysql.connect(host=host_name,user=user,passwd=passwd,db=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading sql table into pandas DataFrame\n",
    "tweet_df = pd.read_sql_query('''select * from tweets_locations2''', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = tweet_df.drop(columns=['tweet_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to clean the tweets\n",
    "def remove_punc(message):\n",
    "    Test_punc_removed = [ char for char in message if char not in string.punctuation]\n",
    "    Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "    return Test_punc_removed_join\n",
    "tweet_df['tweet'] = tweet_df['tweet'].apply(remove_punc)\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','tweet','today','week','year', 'https','httpstco', 'rt', 'RT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and remove short words (less than 2 characters)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stop_words and len(token) >= 2:\n",
    "            result.append(token)\n",
    "    return result\n",
    "tweet_df['tweet'] = tweet_df['tweet'].apply(preprocess).apply(lambda x: \" \".join(x))\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Creating two new columns Subjectivity, Polarity\n",
    "tweet_df['Subjectivity'] = tweet_df['tweet'].apply(getSubjectivity)\n",
    "tweet_df['Polarity'] = tweet_df['tweet'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute the negative, neutral and positive analysis\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "tweet_df['Analysis'] = tweet_df['Polarity'].apply(getAnalysis)\n",
    "\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.to_csv(\"TwitterSentimentData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(tweet_df['Analysis']).set_title('Twitter Sentiment Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive sentiment\n",
    "plt.figure(figsize= (20, 20))\n",
    "wc = WordCloud(max_words = 900, width = 1600, height = 800).generate(\" \".join(tweet_df[tweet_df['Polarity'] > 0.0]['tweet']))\n",
    "plt.figure(figsize=(20, 10), facecolor='white', edgecolor='blue')\n",
    "\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nutral sentiment word cloud\n",
    "\n",
    "plt.figure(figsize= (20, 20))\n",
    "wc = WordCloud(background_color='white', max_words = 500, width = 1600, height = 800).generate(\" \".join(tweet_df[tweet_df['Polarity'] == 0.0]['tweet']))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sentiment word cloud\n",
    "\n",
    "plt.figure(figsize= (20, 20))\n",
    "wc = WordCloud(background_color='white', max_words = 500, width = 1600, height = 800).generate(\" \".join(tweet_df[tweet_df['Polarity'] < 0.0]['tweet']))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "# for i in tweet_df['tweet']:\n",
    "for i in tweet_df['tweet'].str.split(' '):\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(list(set(list_of_words)))\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into test and train\n",
    "\n",
    "X = tweet_df['tweet'].str.split(' ')\n",
    "y = tweet_df['Polarity']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer to tokenize the words and create sequences of tokenized words\n",
    "tokenizer = Tokenizer(num_words=total_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Training data\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# Testing data\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The encoding for document\\n', X_train[1:2], 'is:', train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding to training and testing\n",
    "padded_train = pad_sequences(train_sequences, maxlen=29)\n",
    "padded_test = pad_sequences(test_sequences, maxlen=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(padded_train[:3]):\n",
    "    print('The padded encoding for document:',  i+1, 'is', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train, 2)\n",
    "y_test_cat = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, output_dim = 512))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(padded_train, y_train_cat, validation_split=0.05, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "pred = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "# argmax finds the argument that gives the maximum value to find the class with the highest probability(predictions).\n",
    "#index of maximum number\n",
    "prediction = []\n",
    "for i in pred:\n",
    "    prediction.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# list containing original values\n",
    "original = []\n",
    "for i in y_test_cat:\n",
    "    original.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score on text data\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(original, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(original, prediction)\n",
    "sns.heatmap(cm, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
